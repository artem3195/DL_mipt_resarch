{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OM9YH_w6yXD"
   },
   "source": [
    "# Neural network from scratch.\n",
    "\n",
    "Based on YSDA Deep Learning course materials.\n",
    "\n",
    "https://github.com/yandexdataschool/Practical_DL/blob/spring2019/homework01/homework_modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkOjLyq46yXK"
   },
   "source": [
    "We will implement most common layers using `numpy` for matrix operations. \n",
    "\n",
    "The file `modules.py` contains the abstract classes for **module**, **criterion** and **sequential** model class.\n",
    "\n",
    "**Module** is an abstract class which defines the fundamental methods required for training a neural network.\n",
    "\n",
    "**Criterion** class implements a container which can be used to score the models answers. \n",
    "\n",
    "**Sequential** class implements a container which processes `input` data sequentially. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c5OjyPn6yXK"
   },
   "source": [
    "#### Tips and tricks\n",
    "Original `numpy` operation are preferable to the overloaded ones (`+`, `*`, `-`, etc. vs `np.add`, `np.multiply`, `np.substract`). They are less prone to excess memory usage and seem unified with tensor frameworks.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "```\n",
    "a = np.zeros_like(arr1)\n",
    "\n",
    "a = arr1 + arr2 #  bad, we reallocate memory for the result of + operation\n",
    "\n",
    "a = np.zeros_like(arr1)\n",
    "\n",
    "np.add(arr1, arr2, out=a) #  good, we use same allocated memory, no need to call gc\n",
    "```\n",
    "\n",
    "But I won't follow that advice ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-cNJXFsp6yXL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jY5f8KaAdlTx",
    "outputId": "b04b4d4a-86cb-4384-d2ce-adb31168714e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/basic_s21/week0_08_intro_to_DL/modules.py\n",
    "\n",
    "from modules import Module, Sequential, Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2shlktH6yXL"
   },
   "source": [
    "## Impelementing main layers\n",
    "\n",
    "\n",
    "Let's try to implement some layers in numpy. \n",
    "Today we are going to discuss only the most common ones.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Matrix_calculus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaLmLd-7Wfs8"
   },
   "source": [
    "[Post about einsum](https://habr.com/ru/post/544498/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5FB1rBB6yXM"
   },
   "source": [
    "### 1. Linear transform layer\n",
    "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
    "- input:   **`batch_size x n_in`**\n",
    "- output  **`batch_size x n_out`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4Abm9jleWzI"
   },
   "source": [
    "[Backpropagation for a Linear Layer handout](http://cs231n.stanford.edu/handouts/linear-backprop.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1K9RnIq6yXM"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation \n",
    "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
    "    \n",
    "    The module should work with 2D input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "       \n",
    "        # This is a nice initialization\n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_in, n_out))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        ################################################\n",
    "        # your code here \n",
    "        ################################################\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        ################################################\n",
    "        # your code here \n",
    "        ################################################\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        ################################################\n",
    "        # your code here \n",
    "        ################################################\n",
    "            \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RVaSCBkfC8d"
   },
   "source": [
    "### 2. SoftMax\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
    "\n",
    "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument.\n",
    "\n",
    "[Notes on softmax](https://deepnotes.io/softmax-crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8Zy921LURI_",
    "outputId": "fd939e08-7740-49ce-fd8f-282addbd389c"
   },
   "outputs": [],
   "source": [
    "# simple einsum example \n",
    "p = np.arange(6).reshape((2, 3))\n",
    "np.einsum('bi,bj->bij', p, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kih1-sC6yXN"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        ################################################\n",
    "        # start with normalization for numerical stability\n",
    "        # your code here \n",
    "        ################################################\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        ################################################\n",
    "        # First we create for each example feature vector, it's outer product with itself\n",
    "        # ( p1^2  p1*p2  p1*p3 .... )\n",
    "        # ( p2*p1 p2^2   p2*p3 .... )\n",
    "        # ( ...                     )\n",
    "        # your code here \n",
    "        # Second we need to create an (n_feats, n_feats) identity of the feature vector\n",
    "        # ( p1  0  0  ...  )\n",
    "        # ( 0   p2 0  ...  )\n",
    "        # ( ...            )\n",
    "        # your code here \n",
    "        # Then we need to subtract the first tensor from the second\n",
    "        # ( p1 - p1^2   -p1*p2   -p1*p3  ... )\n",
    "        # ( -p1*p2     p2 - p2^2   -p2*p3 ...)\n",
    "        # ( ...                              )\n",
    "        # your code here \n",
    "        ################################################\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgo9jxXQPGRh"
   },
   "source": [
    "### 3. Negative LogLikelihood criterion (numerically unstable)\n",
    "[multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Remember that targets are one-hot encoded. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
    "- input:   **`batch_size x n_feats`** - probabilities\n",
    "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
    "- output: **scalar**## \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ma_X5zSGXWKb"
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterionUnstable(Criterion):\n",
    "    EPS = 1e-15\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterionUnstable, self)\n",
    "        super(ClassNLLCriterionUnstable, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target):   \n",
    "        ################################################\n",
    "        # Use clipping trick to avoid numerical errors\n",
    "        # your code here \n",
    "        ################################################\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        ################################################\n",
    "        # Use clipping trick to avoid numerical errors\n",
    "        # your code here \n",
    "        ################################################\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterionUnstable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUvohjyp6yXP"
   },
   "outputs": [],
   "source": [
    "def simple_sgd(variables, gradients, config, state):  \n",
    "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
    "    state.setdefault('accumulated_grads', {})\n",
    "    \n",
    "    var_index = 0 \n",
    "    for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
    "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
    "            \n",
    "            current_var -= config['learning_rate'] * current_grad\n",
    "            var_index += 1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2TE4a3-6yXP"
   },
   "source": [
    "## Toy training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ewrE7tb6yXP"
   },
   "source": [
    "Use this example to debug your code, just logistic regression. You do not need to change anything here. This code is provided for you to test the layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "JlD2ypxA6yXP",
    "outputId": "9daae47c-a463-4eec-c87e-1c8cbe478748",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "N = 500\n",
    "\n",
    "X1 = np.random.randn(N,2) + np.array([2,2])\n",
    "X2 = np.random.randn(N,2) + np.array([-2,-2])\n",
    "\n",
    "Y = np.concatenate([np.ones(N),np.zeros(N)])[:,None]\n",
    "Y = np.hstack([Y, 1-Y])\n",
    "\n",
    "X = np.vstack([X1,X2])\n",
    "plt.scatter(X[:,0],X[:,1], c = Y[:,0], edgecolors= 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YA59tRe6yXQ"
   },
   "source": [
    "Define a **logistic regression** for debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBkFEbVA6yXR",
    "outputId": "d8871bf6-4514-49b7-fbb5-6241d8448c14"
   },
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(2, 2))\n",
    "\n",
    "net.add(SoftMax())\n",
    "criterion = ClassNLLCriterionUnstable()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TmMYZrF6yXR"
   },
   "source": [
    "Start with batch_size = 1000 to make sure every step lowers the loss, then try stochastic version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eG4lPvr6yXR"
   },
   "outputs": [],
   "source": [
    "# Iptimizer params\n",
    "optimizer_config = {'learning_rate' : 1e-1}\n",
    "optimizer_state = {}\n",
    "\n",
    "# Looping params\n",
    "n_epoch = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wfDdxLc6yXR"
   },
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def get_batches(dataset, batch_size):\n",
    "    X, Y = dataset\n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_idx = indices[start:end]\n",
    "    \n",
    "        yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_ro0Qqd6yXS"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kjj3f5Q6yXS"
   },
   "source": [
    "Basic training loop. Examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "uHr99fks6yXS",
    "outputId": "7009fc8b-6dc8-469b-f6a2-07ad5200401f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
    "        \n",
    "        net.zeroGradParameters()\n",
    "        \n",
    "        # Forward\n",
    "        predictions = net.forward(x_batch)\n",
    "        loss = criterion.forward(predictions, y_batch)\n",
    "    \n",
    "        # Backward\n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        \n",
    "        # Update weights\n",
    "        simple_sgd(net.getParameters(), \n",
    "                   net.getGradParameters(), \n",
    "                   optimizer_config,\n",
    "                   optimizer_state)      \n",
    "        \n",
    "        loss_history.append(loss)\n",
    "\n",
    "    # Visualize\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "        \n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"#iteration\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Current loss: %f' % loss)    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week08_nn_from_scratch--train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "work_env",
   "language": "python",
   "name": "work_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
